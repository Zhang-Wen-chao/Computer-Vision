{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKJZF7NrjyBP"
   },
   "source": [
    "# PyTorch Tutorial 1: LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN2efcHIZwxv"
   },
   "outputs": [],
   "source": [
    "# import some necessary packages\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import mnist\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwR5wAkco2Sb"
   },
   "source": [
    "# Model Architecture\n",
    "![LeNet-5 Model Architecture](https://image.jiqizhixin.com/uploads/editor/5769eae8-69fd-4259-b1d7-3da4499b7037/1525720940926.jpeg)\n",
    "\n",
    "### Note: the image size of MNIST dataset is 3 $\\times$ 28 $\\times$ 28, not 3 $\\times$ 32 $\\times$ 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWAev7I-Zwx2"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # INPUT -> C1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # C1 -> S2\n",
    "        x = self.pool1(x)\n",
    "        # S2 -> C3\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # C3 -> S4\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # S4 -> C5\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # C5 -> F6\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # F6 -> OUTPUT\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76_DszhKZwx4",
    "outputId": "158bf8c5-1d03-44cc-b2fd-07151c2732f2"
   },
   "outputs": [],
   "source": [
    "# you can try to adjust the following hyperparameters,\n",
    "# and find the performance change\n",
    "lr = 1e-2\n",
    "all_epoch = 20\n",
    "batch_size = 256\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# if you find the result is 'cpu' but you indeed have a GPU device,\n",
    "# please check your code, Nvidia driver and conda environment\n",
    "print(f'Current device: {device}')\n",
    "\n",
    "# data pipeline\n",
    "train_dataset = mnist.MNIST(\n",
    "    root='./train',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataset = mnist.MNIST(\n",
    "    root='./test',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "# move the model into GPU (or CPU)\n",
    "model = Model().to(device)\n",
    "# optimizer\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "# loss function\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "X_Q2jYszZwx6",
    "outputId": "1d879c2e-95bd-4e0c-fe34-e3036d93b18f"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for current_epoch in range(all_epoch):\n",
    "    # training\n",
    "    model.train()\n",
    "    for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "        train_x = train_x.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "\n",
    "        predict_y = model(train_x.float())\n",
    "        # For CrossEntropyLoss, the target must belong to torch.LongTensor\n",
    "        # so we need the method `long()`\n",
    "        loss = loss_fn(predict_y, train_label.long())\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # back prop\n",
    "        loss.backward()\n",
    "        # update model params\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # evaluation\n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    # Note:\n",
    "    # we must start the evaluation mode in the eval phase\n",
    "    # because gradient computation is not allowed in evaluation\n",
    "    model.eval()\n",
    "    for idx, (test_x, test_label) in enumerate(test_loader):\n",
    "        test_x = test_x.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "\n",
    "        predict_y = model(test_x.float()).detach()\n",
    "        predict_y =torch.argmax(predict_y, dim=-1)\n",
    "\n",
    "        current_correct_num = (predict_y == test_label)\n",
    "        all_correct_num += np.sum(current_correct_num.to('cpu').numpy(), axis=-1)\n",
    "        all_sample_num += current_correct_num.shape[0]\n",
    "    acc = all_correct_num / all_sample_num\n",
    "\n",
    "    print(f'Epoch [{current_epoch+1}/{all_epoch}], accuracy: {acc:.3f}')\n",
    "\n",
    "    if not os.path.isdir('models'):\n",
    "        os.mkdir('models')\n",
    "    torch.save(model, f'models/mnist_{acc:.3f}.pt')\n",
    "\n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mg_IrRDkZwx-"
   },
   "outputs": [],
   "source": [
    "# loss curve visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Checkpoints\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJXgav9KZwx_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
